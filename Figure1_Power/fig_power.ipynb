{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure: How large should effect sizes be in neuroimaging to have sufficient power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification of alternative\n",
    "\n",
    "In a brain map in an MNI template, with smoothness of 3 times the voxelsize, there is one active region with voxelwise effect size D.  The (spatial) size of the region is relatively small (<200 voxels).  We want to know how large D should be in order to have 80% power to detect the region using voxelwise FWE thresholding using Random Field Theory.\n",
    "Detect the region means that the maximum in the activated area exceeds the significance threshold.\n",
    "\n",
    "### Strategy\n",
    "\n",
    "1. Compute the voxelwise threshold for the specified smoothness and volume \n",
    "    * _FweThres = 5.12_\n",
    "2. Define the alternative hypothesis, so that the omnibus power is 80% \n",
    "3. How large should the maximum statistic in a (small) region be to exceed the voxelwise threshold with 0.8 power? \n",
    "    * _muMax = 4.00_\n",
    "5. How does this voxel statistic translate to Cohen's D for a given sample size?\n",
    "    * _See Figure_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from __future__ import division\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from neuropower import peakdistribution\n",
    "import scipy.integrate as integrate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import palettable.colorbrewer as cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the voxelwise threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReselSize: 27\n",
      "Volume: 228483\n",
      "ReselCount: 8462.33333333\n",
      "------------\n",
      "FWE voxelwise GRF threshold: 5.123062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From smoothness + mask to ReselCount\n",
    "\n",
    "FWHM = 3\n",
    "ReselSize = FWHM**3\n",
    "MNI_mask = nib.load(\"MNI152_T1_2mm_brain_mask.nii.gz\").get_data()\n",
    "Volume = np.sum(MNI_mask)\n",
    "ReselCount = Volume/ReselSize\n",
    "\n",
    "print(\"ReselSize: \"+str(ReselSize))\n",
    "print(\"Volume: \"+str(Volume))\n",
    "print(\"ReselCount: \"+str(ReselCount))\n",
    "print(\"------------\")\n",
    "\n",
    "# From ReselCount to FWE treshold\n",
    "\n",
    "FweThres_cmd = 'ptoz 0.05 -g %s' %ReselCount\n",
    "FweThres = os.popen(FweThres_cmd).read()\n",
    "\n",
    "print(\"FWE voxelwise GRF threshold: \"+str(FweThres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition of alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect 1 region\n",
    "We define a 'success' as a situation in which the maximum in the active field exceeds\n",
    "the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Power = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How large statistic in a field be to exceed the threshold with power 0.80?\n",
    "\n",
    "We quantify this by computing the expected local maximum in the field (which is a null field elevated by value D).\n",
    "We use the distribution of local maxima of Cheng&Schwartzman to compute the power/effect size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "muRange = np.arange(1.8,5,0.01)\n",
    "muSingle = []\n",
    "for muMax in muRange:\n",
    "# what is the power to detect a maximum \n",
    "    power = 1-integrate.quad(lambda x:peakdistribution.peakdens3D(x,1),-20,float(FweThres)-muMax)[0]\n",
    "    if power>Power:\n",
    "        muSingle.append(muMax)\n",
    "        break\n",
    "print(\"The power is sufficient for one region if mu equals: \"+str(muSingle[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. From the required voxel statistic to Cohen's D for a given sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "Data = pd.read_csv(\"neurosynth_sampsizedata.txt\",sep=\" \",header=None,names=['year','n'])\n",
    "Data['source']='Tal'\n",
    "Data=Data[Data.year!=1997] #remove year with 1 entry\n",
    "David = pd.read_csv(\"david_sampsizedata.txt\",sep=\" \",header=None,names=['year','n'])\n",
    "David['source']='David'\n",
    "Data=Data.append(David)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Data from neurosynth: \"+ str(len(Data[Data['source']==\"Tal\"])))\n",
    "print(\"Data from David: \"+ str(len(Data[Data['source']==\"David\"])))\n",
    "print(\"Data total: \"+ str(len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add detectable effect\n",
    "Data['deltaSingle']=muSingle[0]/np.sqrt(Data['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add jitter for figure\n",
    "stdev = 0.01*(max(Data.year)-min(Data.year))\n",
    "Data['year_jitter'] = Data.year+np.random.randn(len(Data))*stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute medians per year (for smoother)\n",
    "Medians = pd.DataFrame({'year':\n",
    "                     np.arange(start=np.min(Data.year),stop=np.max(Data.year)+1),\n",
    "                      'TalMdSS':'nan',\n",
    "                     'DavidMdSS':'nan',\n",
    "                      'TalMdDSingle':'nan',\n",
    "                     'DavidMdDSingle':'nan',\n",
    "                        'MdSS':'nan',\n",
    "                        'DSingle':'nan'\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yearInd in (range(len(Medians))):\n",
    "    # Compute medians for Tal's data\n",
    "    yearBoolTal = np.array([a and b for a,b in zip(Data.source==\"Tal\",Data.year==Medians.year[yearInd])])\n",
    "    Medians.TalMdSS[yearInd] = np.median(Data.n[yearBoolTal])\n",
    "    Medians.TalMdDSingle[yearInd] = np.median(Data.deltaSingle[yearBoolTal])\n",
    "    # Compute medians for David's data\n",
    "    yearBoolDavid = np.array([a and b for a,b in zip(Data.source==\"David\",Data.year==Medians.year[yearInd])])\n",
    "    Medians.DavidMdSS[yearInd] = np.median(Data.n[yearBoolDavid])\n",
    "    Medians.DavidMdDSingle[yearInd] = np.median(Data.deltaSingle[yearBoolDavid])\n",
    "    # Compute medians for all data\n",
    "    yearBool = np.array(Data.year==Medians.year[yearInd])\n",
    "    Medians.MdSS[yearInd] = np.median(Data.n[yearBool])\n",
    "    Medians.DSingle[yearInd] = np.median(Data.deltaSingle[yearBool])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Medians[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Medians[Medians.year==2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add logscale\n",
    "\n",
    "Medians['MdSSLog'] = [np.log(x) for x in Medians.MdSS]\n",
    "Medians['TalMdSSLog'] = [np.log(x) for x in Medians.TalMdSS]\n",
    "Medians['DavidMdSSLog'] = [np.log(x) for x in Medians.DavidMdSS]\n",
    "\n",
    "Data['nLog']= [np.log(x) for x in Data.n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The figure per List (Tal or David)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twocol = cb.qualitative.Paired_12.mpl_colors\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(12,5))\n",
    "fig.subplots_adjust(hspace=.5,wspace=.3)\n",
    "axs=axs.ravel()\n",
    "axs[0].plot(Data.year_jitter[Data.source==\"Tal\"],Data['nLog'][Data.source==\"Tal\"],\"r.\",color=twocol[0],alpha=0.5,label=\"\")\n",
    "axs[0].plot(Data.year_jitter[Data.source==\"David\"],Data['nLog'][Data.source==\"David\"],\"r.\",color=twocol[2],alpha=0.5,label=\"\")\n",
    "axs[0].plot(Medians.year,Medians.TalMdSSLog,color=twocol[1],lw=3,label=\"Neurosynth\")\n",
    "axs[0].plot(Medians.year,Medians.DavidMdSSLog,color=twocol[3],lw=3,label=\"David et al.\")\n",
    "axs[0].set_xlim([1998,2016])\n",
    "axs[0].set_ylim([0,8])\n",
    "axs[0].set_xlabel(\"Year\")\n",
    "axs[0].set_ylabel(\"Median Sample Size\")\n",
    "axs[0].legend(loc=\"upper left\",frameon=False)\n",
    "#labels=[1,5,10,20,50,150,500,1000,3000]\n",
    "labels=[1,4,16,64,256,1024,3000]\n",
    "axs[0].set_yticks(np.log(labels))\n",
    "axs[0].set_yticklabels(labels)\n",
    "\n",
    "\n",
    "axs[1].plot(Data.year_jitter[Data.source==\"Tal\"],Data.deltaSingle[Data.source==\"Tal\"],\"r.\",color=twocol[0],alpha=0.5,label=\"\")\n",
    "axs[1].plot(Data.year_jitter[Data.source==\"David\"],Data.deltaSingle[Data.source==\"David\"],\"r.\",color=twocol[2],alpha=0.5,label=\"\")\n",
    "axs[1].plot(Medians.year,Medians.TalMdDSingle,color=twocol[1],lw=3,label=\"Neurosynth\")\n",
    "axs[1].plot(Medians.year,Medians.DavidMdDSingle,color=twocol[3],lw=3,label=\"David et al.\")\n",
    "axs[1].set_xlim([1998,2016])\n",
    "axs[1].set_ylim([0,3])\n",
    "axs[1].set_xlabel(\"Year\")\n",
    "axs[1].set_ylabel(\"Effect Size with 80% power\")\n",
    "axs[1].legend(loc=\"upper right\",frameon=False)\n",
    "plt.savefig('samplesize_and_power.pdf',dpi=600)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print median sample size and power for Neurosynth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Medians.loc[:, lambda df: ['year', 'TalMdSS', 'TalMdDSingle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
